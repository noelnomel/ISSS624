---
title: "In-Class Exercise 2"
date: "25 November 2023"
date-modified: "last-modified"
format: html
execute: 
  echo: true # all code chunks will appear
  eval: true # all code chunks will run live
  warning: false # do not display warning message
editor: visual
---

# 1. Overview

This in-class exercise introduces `sfdep`, an alternative R package to `spdep` used in Hands-on Exercise 2. It will cover methods to derive spatial weights, global and and local measures of spatial association, and emerging hotspot analysis.

# 2. Getting Started

We will use the following packages in this in-class exercise:

```{r}
pacman::p_load(sf, tmap, sfdep, tidyverse, knitr, plotly)
```

# 2.1. The Data

We will use two data sets about Hunan in this exercise.

First, we will import Hunan, a geospatial data set in ESRI shapefile format:

```{r}
hunan <- st_read(dsn = "data/geospatial",
                 layer = "Hunan")
```

Next, we will import Hunan_2012, an attribute data set in csv file:

```{r}
hunan2012 <- read_csv("data/aspatial/Hunan_2012.csv")
```

We will combine hunan and hunan2012 using left_join():

```{r}
hunan_GDPPC <- left_join(hunan, hunan2012) %>%
  select(1:4, 7, 15)

glimpse(hunan_GDPPC)
```

> Use hunan as the first data in `left_join()` to retain the geometry of the shapefile and the sf data frame output format.

> `select()` is used to retain the columns we want in the `hunan_GDPPC` simple feature data frame.

Lastly, we will import Hunan_GDPPC, an attribute data set in csv file:

```{r}
GDPPC <- read_csv("data/aspatial/Hunan_GDPPC.csv")
```

## 2.2. Visualising GDPPC by County

```{r}
tmap_mode("plot")
tm_shape(hunan_GDPPC) +
  tm_fill("GDPPC", style = "quantile", palette = "Blues") +
  tm_borders(alpha = 0.5) +
  tm_layout(main.title = "Distirbution of GDP Per Capita by County, Hunan Province",
            main.title.position = "center",
            main.title.size = 0.8,
            legend.width = 0.45,
            legend.height = 0.45,
            frame = TRUE) + 
  tm_scale_bar() +
  tm_grid(alpha = 0.15)
  
```

# 3. Spatial Weights

Spatial weights matrices are mathematical representations of the spatial structure of the data. They assign weights to the pairs of locations based on some criteria, such as distance, adjacency, or similarity. Two types of spatial weights will be discussed in this exercise:

-   Contiguity weights

-   Distance-based weights

# 4. Contiguity Spatial Weights

Contiguity spatial weights assign weights based on whether pairs of locations share a common boundary or vertex. Contiguity can be derived using different methods, such as Queen, Rook, Bishop.

Two steps are required to derive contiguity spatial weights using the sfdep package:

1.  Identify a contiguity neighbour list using `st_contiguity()`

2.  Derive contiguity spacial weights using `st_weights()`

## 4.1. Using Queen's Method

First, we will use the following code to derive a contiguity neighbour list:

```{r}
nb_queen <- hunan_GDPPC %>%
  mutate(nb = st_contiguity(geometry),
         .before = 1)

summary(nb_queen$nb)
```

It is observed from the summary that there are 88 regions in Hunan province. The most well-connected region has 11 neighbours. There are two areas with only one neighbour.

```{r}
kable(head(nb_queen, 
           n = 10))
```

We can use `st_ng_lag_cumul()` to identify higher order neighbours:

```{r}
nb2_queen <- hunan_GDPPC %>%
  mutate(nb = st_contiguity(geometry),
         nb2 = st_nb_lag_cumul(nb,2),
         .before = 1)

nb2_queen
```

| When order is 2, the result contains the first and second order neighbours.

We can derive the contiguity weights in one step using the following code:

```{r}
wm_q <- hunan_GDPPC %>%
  mutate(nb = st_contiguity(geometry),
         wt = st_weights(nb, style = "W"),
         .before = 1)
wm_q
```

Three arguments can be passed into st_weights:

-   `nb`: A neighbor list object as created by `st_neighbors()`.

-   `style`: Default "W" for row standardized weights. This value can also be "B", "C", "U", "minmax", and "S". See spdep::nb2listw() for details.

-   `allow_zero`: If `TRUE`, assigns zero as lagged value to zone without neighbors.

## 4.2. Using Rook's Method

First, we will use the following code to derive a contiguity neighbour list:

```{r}
nb_rook <- hunan_GDPPC %>%
  mutate(nb = st_contiguity(geometry,
                            queen = FALSE),
         .before = 1)

summary(nb_rook$nb)
```

We can derive the contiguity weights in one step using the following code:

```{r}
wm_r <- hunan_GDPPC %>%
  mutate(nb = st_contiguity(geometry,
                            queen = FALSE),
         wt = st_weights(nb, style = "W"),
         .before = 1)
wm_r
```

# 5. Distance-based Weights

Distance-based weights are the most common type of spatial weights matrices. They assign higher weights to pairs of locations that are closer to each other and lower weights to pairs that are farther apart. There are three commonly-used distance-based spatial weights:

1.  Fixed distance weight

2.  Adaptive distance weight

3.  Inverse distance weight

## 5.1. Deriving Fixed Distance Weight

First, we need to determine the upper limit for the distance band by using the following steps:

```{r}
geo <- sf::st_geometry(hunan_GDPPC)
nb <- st_knn(geo, longlat = TRUE)
dists <- unlist(st_nb_dists(geo,nb))

summary(dists)
```

The summary report shows that the maximum distance to the nearest neighbour is 65.80 km. Hence a threshold of 66km will ensure that each neighbour has at least one neighbour.

Next, we will derive the fixed distance weight using the following code:

```{r}
wm_fd <- hunan_GDPPC %>%
  mutate(nb = st_dist_band(geometry,
                           upper = 66),
         wt = st_weights(nb),
         .before = 1)
```

> `st_dists_band()` is used to identify neigbours based on a distance band (66km in our case). The output is a list of neighbours saved as `nb`.
>
> `st_weights()` is used to calculate polygon spatial weights of the `nb` list. Note that:
>
> -   The default style argument is set to "W" for row-standardised weights
>
> -   Tthe default `allow_zero` argument is set to `TRUE` to assign zero as lagged value to zones without neighbours.

```{r}
kable(head(wm_fd, 
           n = 10))
```

## 5.2. Deriving Adaptive Distance Weights

We will derive the adaptive distance weight using the following code:

```{r}
wm_ad <- hunan_GDPPC %>%
  mutate(nb = st_knn(geometry,
                     k = 8),
         wt = st_weights(nb),
         .before = 1)

kable(head(wm_ad, 
           n = 10))
```

> -   `st_knn()` is used to identify neighbours based on `k` number of nearest neighbours (8 in our case). The output is a list of neighbours saved as `nb`.

## 5.3. Deriving Inverse Distance Weights

We will derive the inverse distance weight using the following code:

```{r}
wm_idw <- hunan_GDPPC %>%
  mutate(nb = st_contiguity(geometry),
         wt = st_inverse_distance(nb, geometry,
                                  scale = 1,
                                  alpha = 1),
         .before = 1)

kable(head(wm_idw, 
           n = 10))
```

> -   `st_inverse_distance()` is used to calculate the inverse distance weights of neighbours on the nb list generated from `st_contiguity()`.

# 6. Global and Local Measures of Spatial Association

## 6.1. Global Moran's I

### 6.1.1. Computing Global Moran's I Statistics

We will use the following code to compute the Moran's I value using contiguity weight derived from the Queen's method.

```{r}
moranI <- global_moran(wm_q$GDPPC,
                       wm_q$nb,
                       wm_q$wt)

glimpse(moranI)
```

The output from `sfdep` package is a tibble data.frame.

### 6.1.2. Performing Global Moran's I Test

Instead of computing Moran's I statistics, **Moran's I test** is generally performed. We can perform the test using the following code:

```{r}
global_moran_test(wm_q$GDPPC, wm_q$nb, wm_q$wt)
```

> -   The default for alternative argument is "two.sided". Other arguments are "greater" or "less"
> -   By default, the randomisation argument is TRUE.

### 6.1.3. Performing Global Moran I's Permutation Test

**Monte Carl**o simulation should be used to perform the statistical test.

First, we use `set.seed()` to ensure that the computation is reproducible.

```{r}
set.seed(1234)
```

We use the following code to perform the Monte Carlo simulation:

```{r}
global_moran_perm(wm_q$GDPPC, wm_q$nb, wm_q$wt,
                  nsim = 99)
```

The report shows that the p-value is less than alpha value of 0.05. We reject the null hypothesis that the spatial patterns are spatially independent. As Moran's I statistics is greater than 0, we can infer that the spatial distribution shows signs of clustering.

| Note: The total simulations conducted is nsim + 1. When nsim = 99, 100 simulations will be performed.

# 6.2. Local Moran's I

### 6.2.1. Computing Local Moran's I

We will use the following code to compute Local Moran's I on GDPPC at the county level, using the `local_moran()` function from `sfdep`. Contiguity weights using the Queen's method is used in the code below:

```{r}
lisa <- wm_q %>%
  mutate(local_moran = local_moran(GDPPC, nb, wt, nsim = 99),
         .before = 1) %>%
  unnest(local_moran)
lisa
```

The output of `local_moran()` is a sf data frame containing the following columns:

-   `ii`: local moran statistic

-   `eii`: expectation of local moran statistic

-   `var_ii`: variance of local moran statistic

-   `z_ii`: standard deviate of local moran statistic

-   `p_ii`: p-value of local moran statistic using `pnorm()`

-   `p_ii_sim`: `rank()` and `punif()` of observed rank for \[0,1\] values for localmoran_perm()

-   `p_folded_sim`: simulation folded \[0,0.5\] range ranked p-value based on crand.py of pysal

> `unnest()` is used to expand a list-column containing data frames into rows and columns.
>
> `sfdep` helps to compute the low-low, low-high, high-low, and high-high by mean, median and pysal. Generally, mean is used for spatial clustering but median should be used if distribution is skewed.

### 6.2.1. Visualising Local Moran's I

```{r}
tmap_mode("plot")
tm_shape(lisa) +
  tm_fill("ii") + 
  tm_borders(alpha = 0.5) +
  tm_view(set.zoom.limits = c(6,8)) +
  tm_layout(main.title = "Local Moran's I of GDPPC",
            main.title.size = 0.8)
```

### 6.2.2. Visualisiing p-value of Local Moran's I

```{r}
tmap_mode("plot")
tm_shape(lisa) +
  tm_fill("p_ii_sim") + 
  tm_borders(alpha = 0.5) +
   tm_layout(main.title = "p-value of local Moran's I",
            main.title.size = 0.8)
```

#### Comparison

```{r}
tmap_mode("plot")
moranI_map <- tm_shape(lisa) +
  tm_fill("ii") + 
  tm_borders(alpha = 0.5) +
  tm_view(set.zoom.limits = c(6,8)) +
  tm_layout(main.title = "Local Moran's I of GDPPC",
            main.title.size = 0.8)

pvalue_map <- tm_shape(lisa) +
  tm_fill("p_ii_sim") + 
  tm_borders(alpha = 0.5) +
   tm_layout(main.title = "p-value of local Moran's I",
            main.title.size = 0.8)

tmap_arrange(moranI_map, pvalue_map, ncol = 2)
```

### 6.2.3. Visualising LISA Map

LISA map is a categorical map showing outliers and clusters. There are two types of outliers: High-Low and Low-High. There are two types of clusters: High-High and Low-Low. LISA map is an interpreted map by combining local Moran's I of geographical areas and their respective p-values.

In LISA sf data.frame, we can find three fields with LISA categories: mean, median and pysal. In general, classification using mean will be used:

```{r}
lisa_sig <- lisa  %>%
  filter(p_ii_sim < 0.05)
tmap_mode("plot")
tm_shape(lisa) +
  tm_polygons() +
  tm_borders(alpha = 0.5) +
tm_shape(lisa_sig) +
  tm_fill("mean") + 
  tm_borders(alpha = 0.4)
```

# 7 Hot Spot and Cold Spot Area Analysis (HCSA)

HCSA uses spatial weights to identify locations of statistically significant hot spots and cold spots in a spatially weighted attribute that are in proximity to one another based on calculated distance. The analysis groups features when similar high (hot) or low (cold) values are found in a cluster. The polygon features usually represent administration boundaries or a custom grid structure.

## 7.1. Computing Local Gi\* Statistics

We will use the following code to compute local Gi\* statistics based on the inverse distance weights matrix:

```{r}
HCSA <- wm_idw %>%
  mutate(local_Gi = local_gstar_perm(
    GDPPC, nb, wt, nsim = 499),
    .before = 1) %>%
  unnest(local_Gi)

HCSA
```

## 7.2. Visualising Gi\*

```{r}
tmap_mode("plot")
tm_shape(HCSA) +
  tm_fill("gi_star") +
  tm_borders(alpha = 0.5) +
  tm_view(set.zoom.limits = c(6,8))
```

## 7.3. Visualising p-value of HCSA

```{r}
tmap_mode("plot")
tm_shape(HCSA) +
  tm_fill("p_sim") +
  tm_borders(alpha = 0.5)
```

#### Comparison

```{r}
tmap_mode("plot")
gistat_map <- tm_shape(HCSA) +
  tm_fill("gi_star") +
  tm_borders(alpha = 0.5) +
  tm_view(set.zoom.limits = c(6,8))

pvalue_hcsa <- tm_shape(HCSA) +
  tm_fill("p_sim") +
  tm_borders(alpha = 0.5)

tmap_arrange(gistat_map, pvalue_hcsa, ncol = 2)
```

## 7.4. Visualing Hot Spot and Cold Spot Areas

We will plot the significant (p-value \< 0.05) hot spot and cold spot areas:

```{r}
HCSA_sig <- HCSA %>%
  filter(p_sim < 0.05)

tmap_mode("plot")
tm_shape(HCSA) +
  tm_polygons() +
  tm_borders(alpha = 0.5) +
tm_shape(HCSA_sig) +
  tm_fill("gi_star") +
  tm_borders(alpha = 0.4)
```

The map above shows that there is one hot spot area and two cold spot areas. The hot spot area coincides with the High-High cluster identified using local Moran's I method.

# 8. Emerging Hot Spot Analysis

Emerging Hot Spot Analysis (EHSA) is a spatio-temporal analysis method to reveal and describe how hot spot and cold spots evolve over time.

## 8.1. Creating a Time Series Cube

```{r}
GDPPC_st <- spacetime(GDPPC, hunan,
                      .loc_col = "County",
                      .time_col = "Year")
GDPPC_st
```

We can use `is_spacetime_cube()` to verify if `GDPPC_st` is a space-time cube object:

```{r}
is_spacetime_cube(GDPPC_st)
```

## 8.2. Deriving Spatial Weights

We will identify neighbours and derive the inverse distance weights using the following code:

```{r}
GDPPC_nb <- GDPPC_st %>%
  activate("geometry") %>%
  mutate(nb = include_self(st_contiguity(geometry)),
         wt = st_inverse_distance(nb, geometry, 
                                  scale = 1, alpha = 1),
         .before = 1) %>%
  set_nbs("nb") %>%
  set_wts("wt")
head(GDPPC_nb)
```

> -   `activate()` from the dplyr package is used to activate the geometry context.
> -   `mutate()` from the dplyr package is used to create new columns `nb` and `wt`.
> -   `set_nbs()` and `set_wts()` is used to activate the data and copy `nb` and `wt` columns to each time-slicing
>     -   **Do not** rearrange the observations after using `set_nbs()` or `set_wts()`.

## 8.3. Computing Gi\*

To calculate the local Gi\* for each location, we can group the columns by Year and apply `local_gstar_perm()`. After which, we can use `unnest()` to unnest the gi_star column of the newly created gi_stars data.frame.

```{r}
gi_stars <- GDPPC_nb %>%
  group_by(Year) %>%
  mutate(gi_star = local_gstar_perm(
    GDPPC, nb, wt)) %>%
  unnest(gi_star)
```

## 8.4. Mann-Kendall Test

We can then evaluate each location for a trend using the Mann-Kendall test. The code chunk below uses Changsha county:

```{r}
cbg <- gi_stars %>%
  ungroup() %>%
  filter(County == "Changsha") |>
  select(County, Year, gi_star)
```

Next, we plot the results using `ggplot2` functions:

```{r}
ggplot(data = cbg, 
       aes(x = Year,
           y = gi_star)) +
  geom_line() +
  theme_light()
```

An interactive plot can be recreated using `ggplotly()` of the `plotly` package.

```{r}
p <- ggplot(data = cbg, 
       aes(x = Year,
           y = gi_star)) +
  geom_line() +
  theme_light()

ggplotly(p)
```

```{r}
cbg %>%
  summarise(mk = list(
    unclass(
      Kendall::MannKendall(gi_star)))) %>%
  unnest_wider(mk)
```

In the results, `sl` is the p-value. The results show that there is a slight upward trend but it is insignificant.

The test can be replicated for each County using `group_by()`:

```{r}
ehsa <- gi_stars %>%
  group_by(County) %>%
  summarise(mk = list(
    unclass(
      Kendall::MannKendall(gi_star)))) %>%
  tidyr::unnest_wider(mk)
```

We can use the following code to show significant emerging hot or cold spots:

```{r}
emerging <- ehsa %>%
  arrange(sl, abs(tau)) %>%
  slice(1:5)
```

## 8.5. Performing Emerging Hotspot Analysis (EHSA)

We will perform EHSA by using `emerging_hotspot_analysis()` of the `sfdep` package. It takes the following arguments:

-   x = a spacetime object

-   .var = the quoted name of the variable of interest

-   k = number of time lags, default is 1

-   nsim = number of simulations to perform

```{r}
ehsa <- emerging_hotspot_analysis(
  x = GDPPC_st, 
  .var = "GDPPC",
  k = 1, 
  nsim = 99
)
```

## 8.6. Visualising Distribution of EHSA Classes

Using `ggplot2` functions, we will plot the distribution of EHSA classes as a bar chart:

```{r}
ggplot(data = ehsa, 
       aes(x = classification)) + 
  geom_bar()
```

The bar chart reveals that sporadic coldspot class the highest number of counties.

## 8.7. Visualsing EHSA

Before we can visualise the geographic distribution of EHSA classes, we have to join the hunan and ehsa datasets together:

```{r}
hunan_ehsa <- hunan %>%
  left_join(ehsa, 
            by = join_by(County == location))
```

We use `tmap` functions to plot a choropleth map:

```{r}
ehsa_sig <- hunan_ehsa %>%
  filter(p_value < 0.05) 

tmap_mode("plot")
tm_shape(hunan_ehsa) + 
  tm_polygons() +
  tm_borders(alpha = 0.5) +
tm_shape(ehsa_sig) +
  tm_fill("classification") +
  tm_borders(alpha = 0.4)
```
